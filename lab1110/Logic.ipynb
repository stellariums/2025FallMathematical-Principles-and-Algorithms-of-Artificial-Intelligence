{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归实验\n",
    "\n",
    "## Tips\n",
    "- 实验前查看issue，同学们有问题可以在issue中提出，同时可以参考过往的问题完成此实验。\n",
    "- 善用搜索引擎、AIGC工具拓展知识面。\n",
    "## Issue\n",
    "[【腾讯文档】逻辑回归-issues](\n",
    "https://docs.qq.com/doc/DWmltZUxvRkNWY0FK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 数据处理\n",
    "\n",
    "为了能够完成各种数据操作，我们需要某种方法来存储和操作数据。 通常，我们需要做两件重要的事：（1）获取数据；（2）将数据读入计算机后对其进行处理。 如果没有某种方法来存储数据，那么获取数据是没有意义的。\n",
    "\n",
    "### 预处理\n",
    "\n",
    "为了能用深度学习来解决现实世界的问题，我们经常从预处理原始数据开始， 而不是从那些准备好的张量格式数据开始。 在Python中常用的数据分析工具中，我们通常使用pandas软件包。 像庞大的Python生态系统中的许多其他扩展包一样，pandas可以与张量兼容。 这一节同学们讲使用pandas预处理原始数据，并将原始数据转换为张量格式的步骤。\n",
    "#### 数据集读取\n",
    "\n",
    "首先，需要尝试使用pandas中的`read_csv`函数读取train, val, test 这三个数据集（保留列名），同时在`utils.py`文件中，助教定义了一个名为`data_utils`的类，可以对读取的pd数据框进行简单的预处理。请尝试调用这个类的方法对于数据集进行进一步的处理。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loanAmnt                0\n",
       "term                    0\n",
       "interestRate            0\n",
       "installment             0\n",
       "grade                   0\n",
       "employmentTitle         0\n",
       "employmentLength      518\n",
       "homeOwnership           0\n",
       "annualIncome            0\n",
       "verificationStatus      0\n",
       "issueDate               0\n",
       "isDefault               0\n",
       "purpose                 0\n",
       "postCode                0\n",
       "regionCode              0\n",
       "dti                     3\n",
       "delinquency_2years      0\n",
       "ficoRangeLow            0\n",
       "ficoRangeHigh           0\n",
       "openAcc                 0\n",
       "pubRec                  0\n",
       "pubRecBankruptcies      3\n",
       "revolBal                0\n",
       "revolUtil              11\n",
       "totalAcc                0\n",
       "initialListStatus       0\n",
       "applicationType         0\n",
       "earliesCreditLine       0\n",
       "title                   0\n",
       "policyCode              0\n",
       "n0                    325\n",
       "n1                    325\n",
       "n2                    325\n",
       "n3                    325\n",
       "n4                    268\n",
       "n5                    325\n",
       "n6                    325\n",
       "n7                    325\n",
       "n8                    325\n",
       "n9                    325\n",
       "n10                   268\n",
       "n11                   624\n",
       "n12                   325\n",
       "n13                   325\n",
       "n14                   325\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 数据集读取\n",
    "# TODO\n",
    "train = pd.read_csv(\"train_sub.csv\")\n",
    "val = pd.read_csv(\"val_sub.csv\")\n",
    "test = pd.read_csv(\"test_sub.csv\")\n",
    "\n",
    "# 数据集预处理\n",
    "import utils\n",
    "# TODO\n",
    "train = utils.data_utils(train).pipeline()\n",
    "val = utils.data_utils(val).pipeline()\n",
    "test = utils.data_utils(test).pipeline()\n",
    "\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理缺失值\n",
    "注意，上一个单元格输出的结果代表着每一列中缺失值的数量。为了处理缺失的数据，典型的方法包括插值法和删除法， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。\n",
    "\n",
    "首先，我们对于需要分析数据的input 和 output 的label，注意到`isDefault`是output的列名，所以首先我们需要丢弃掉所有`isDefault`列为NaN的行。\n",
    "\n",
    "其次，对于其他input为空的行，我们需要使用同一列的均值来对 `NaN`进行填充。\n",
    "\n",
    "##### data.mean()\n",
    "用于计算数据框中每一列的均值。它可以帮助我们快速了解数据的中心趋势，特别是在处理缺失值时，我们可以用均值来填充NaN值。\n",
    "\n",
    "##### pd.fillna()\n",
    "是pandas中用于填充缺失值的方法。它允许我们用特定的值（如均值、中位数或其他值）来替换NaN。使用fillna()可以确保数据的完整性，避免在后续分析中因缺失值而导致的错误。\n",
    "\n",
    "##### pd.dropna()\n",
    "是pandas中用于删除缺失值的方法。它可以根据指定的条件（如某一列是否为NaN）来删除行或列。使用dropna()可以帮助我们清理数据集，确保分析时只使用完整的数据。\n",
    "\n",
    "##### 例如，以下代码展示了如何使用这些方法：\n",
    "```python\n",
    "1. 计算均值并填充缺失值：\n",
    "mean_value = data['column_name'].mean()\n",
    "data['column_name'].fillna(mean_value, inplace=True)\n",
    "\n",
    "2. 删除特定列中含有NaN的行：\n",
    "data.dropna(subset=['column_name'], inplace=True)\n",
    "```\n",
    "\n",
    "<!-- ##### 通过这些方法，我们可以有效地处理数据中的缺失值，提高数据分析的准确性。 -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loanAmnt              0\n",
       "term                  0\n",
       "interestRate          0\n",
       "installment           0\n",
       "grade                 0\n",
       "employmentTitle       0\n",
       "employmentLength      0\n",
       "homeOwnership         0\n",
       "annualIncome          0\n",
       "verificationStatus    0\n",
       "issueDate             0\n",
       "isDefault             0\n",
       "purpose               0\n",
       "postCode              0\n",
       "regionCode            0\n",
       "dti                   0\n",
       "delinquency_2years    0\n",
       "ficoRangeLow          0\n",
       "ficoRangeHigh         0\n",
       "openAcc               0\n",
       "pubRec                0\n",
       "pubRecBankruptcies    0\n",
       "revolBal              0\n",
       "revolUtil             0\n",
       "totalAcc              0\n",
       "initialListStatus     0\n",
       "applicationType       0\n",
       "earliesCreditLine     0\n",
       "title                 0\n",
       "policyCode            0\n",
       "n0                    0\n",
       "n1                    0\n",
       "n2                    0\n",
       "n3                    0\n",
       "n4                    0\n",
       "n5                    0\n",
       "n6                    0\n",
       "n7                    0\n",
       "n8                    0\n",
       "n9                    0\n",
       "n10                   0\n",
       "n11                   0\n",
       "n12                   0\n",
       "n13                   0\n",
       "n14                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 过滤标签（`isDefault`列）为空的行\n",
    "\n",
    "def filter_NA(data):\n",
    "    \"\"\"\n",
    "    删除 `isDefault` 列为 NaN 的行\n",
    "    Args:\n",
    "        data (pd.DataFrame): 输入数据框\n",
    "    Returns:\n",
    "        pd.DataFrame: 过滤后的数据框\n",
    "    \"\"\"\n",
    "    # \n",
    "    data.dropna(subset=[\"isDefault\"],inplace=True)\n",
    "    return data\n",
    "\n",
    "train, val, test = filter_NA(train), filter_NA(val), filter_NA(test)\n",
    "# 填充对于其他input为空的行\n",
    "def fill_mean(data):\n",
    "    \"\"\"\n",
    "    用每列均值填充 NaN\n",
    "    Args:\n",
    "        data (pd.DataFrame): 输入数据框\n",
    "    Returns:\n",
    "        pd.DataFrame: 填充后的数据框\n",
    "    \"\"\"\n",
    "    # \n",
    "    columns_to_fill = [column for column in data.columns if column != 'isDefault']\n",
    "    for col in columns_to_fill:\n",
    "        nan_count = data[col].isnull().sum()\n",
    "        if nan_count>0:\n",
    "            mean=data[col].mean()\n",
    "            data[col].fillna(mean,inplace=True)\n",
    "    return data\n",
    "\n",
    "train, val, test = fill_mean(train), fill_mean(val), fill_mean(test)\n",
    "train.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 转换为张量格式\n",
    "\n",
    "按照之前的划分方式，我们把数据划分成input和label，其中，label为isDefault 列，input为其他列，并且转换为张量(tensor)的形式。\n",
    "\n",
    "\n",
    "##### 1. Pandas数据选择和处理\n",
    "\n",
    "###### 1.1 数据选择\n",
    "在Pandas中，我们可以通过多种方式选择数据：\n",
    "\n",
    "```python\n",
    "# 假设我们有一个数据框df\n",
    "# 1. 选择单列\n",
    "column_data = df['column_name']  # 返回Series\n",
    "# 2. 选择多列\n",
    "columns_data = df[['column1', 'column2']]  # 返回DataFrame\n",
    "```\n",
    "\n",
    "###### 1.2 使用drop删除列\n",
    "`drop()`是一个非常有用的方法，用于删除指定的行或列：\n",
    "\n",
    "```python\n",
    "# 删除列\n",
    "df.drop('column_name', axis=1)  # axis=1表示删除列，axis=0表示删除行\n",
    "\n",
    "# 示例：删除'isDefault'列\n",
    "train_input = train.drop('isDefault', axis=1)\n",
    "```\n",
    "\n",
    "参数说明：\n",
    "- `axis=1`: 表示删除列\n",
    "- `axis=0`: 表示删除行\n",
    "- `inplace=True`: 如果设置为True，则直接在原数据框上修改\n",
    "\n",
    "###### 1.3 .values属性\n",
    "`.values`属性用于将Pandas的DataFrame或Series转换为NumPy数组：\n",
    "\n",
    "```python\n",
    "# DataFrame转换为NumPy数组\n",
    "numpy_array = df.values\n",
    "\n",
    "# 示例\n",
    "train_input_array = train_input.values  # 转换为numpy数组\n",
    "```\n",
    "\n",
    "##### 2. PyTorch张量转换\n",
    "\n",
    "###### 2.1 创建张量\n",
    "`torch.tensor()`用于创建PyTorch张量：\n",
    "\n",
    "```python\n",
    "# 基本语法\n",
    "tensor = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "# 示例：将numpy数组转换为PyTorch张量\n",
    "train_input = torch.tensor(train_input.values, dtype=torch.float32)\n",
    "```\n",
    "\n",
    "主要参数：\n",
    "- `data`: 输入数据（可以是列表、NumPy数组等）\n",
    "- `dtype`: 指定数据类型，常用的有：\n",
    "  - `torch.float32`: 32位浮点数\n",
    "  - `torch.int64`: 64位整数\n",
    "  - `torch.bool`: 布尔值\n",
    "\n",
    "##### 注意事项\n",
    "1. 在转换为张量时，要确保数据类型正确\n",
    "2. 对于分类问题的标签，通常使用`float32`或`long`类型\n",
    "3. 在处理大数据集时，注意内存使用情况\n",
    "4. 使用`drop()`时注意保存结果，因为默认不会修改原数据框\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 数据集划分\n",
    "\n",
    "train_input, train_label = train.drop('isDefault',axis=1),train['isDefault']\n",
    "val_input, val_label =val.drop('isDefault',axis=1),val['isDefault']\n",
    "test_input, test_label =test.drop('isDefault',axis=1),test['isDefault']\n",
    "\n",
    "\n",
    "# 使用MinMaxScaler对训练数据、验证数据和测试数据进行归一化处理\n",
    "# MinMaxScaler将特征缩放到指定的范围（默认是0到1），\n",
    "# 通过这种方式，可以确保每个特征在同一尺度上，从而提高模型的训练效果\n",
    "# 归一化处理有助于加快收敛速度，并减少不同特征之间的影响\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_input = scaler.fit_transform(train_input.values)\n",
    "test_input = scaler.transform(test_input.values)\n",
    "val_input = scaler.transform(val_input.values)\n",
    "train_label = train_label.values\n",
    "test_label = test_label.values\n",
    "val_label = val_label.values\n",
    "\n",
    "# TODO 将数据转换为张量\n",
    "train_input = torch.tensor(train_input,dtype=torch.float32)\n",
    "train_label = torch.tensor(train_label,dtype=torch.float32)\n",
    "test_input = torch.tensor(test_input,dtype=torch.float32)\n",
    "test_label = torch.tensor(test_label,dtype=torch.float32)\n",
    "val_input = torch.tensor(val_input,dtype=torch.float32)\n",
    "val_label = torch.tensor(val_label,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建Pytorch数据集\n",
    "\n",
    "在pytorch中，提供了一种非常方便的数据读取机制，即`torch.utils.data.Dataset`类和`torch.utils.data.Dataloader`类，通过两种方式的组合，我们可以得到数据迭代器，在每次训练中，利用这个迭代器每次输出一组batch数据（即训练数据集中的一个子集）。\n",
    "\n",
    "#### torch.utils.data.Dataset\n",
    "torch.utils.data.Dataset代表着自定义数据集方法的类，用户可以继承这个类来自定义自己的数据集类，继承时，用户需要重载`__len__()`和 `__getitem__()`这两个方法\n",
    "\n",
    "- `__len__()` 返回数据集的大小。 我们构建的数据集也是一个对象，这个方法希望像list,string,tuple等方法一样，可以直接获得对象的大小\n",
    "- `__getitem__()`实现获得数据集中的某一个数据。 list,string,tuple可以通过一个索引来获得序列中的任意元素，通过实现``__getitem__()``，我们希望获得类似的功能。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, *tensors):\n",
    "        assert all(\n",
    "            tensors[0].size(0) == tensor.size(0) for tensor in tensors\n",
    "        ), \"Size mismatch between tensors\"\n",
    "        self.tensor = tensors\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return a tuple of tensors at the given index\n",
    "        # Each tensor in self.tensor is sliced at the index position\n",
    "        # For example, if we have input tensor and label tensor\n",
    "        # This will return (input[index], label[index])\n",
    "        # TODO\n",
    "\n",
    "        return (self.tensor[0][index],self.tensor[1][index])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the length of the dataset\n",
    "        # TODO\n",
    "\n",
    "        return len(self.tensor[0])\n",
    "\n",
    "train_dataset = MyDataset(train_input, train_label)\n",
    "val_dataset = MyDataset(val_input, val_label)\n",
    "test_dataset = MyDataset(test_input, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.utils.data.DataLoader\n",
    "作用：\n",
    "- `DataLoader`将`Dataset`对象或者自定义数据类封装成一个迭代器。\n",
    "- 迭代器可以迭代输出`Dataset`的内容\n",
    "- 可以实现多个进程、shuffle、不同采样策略，数据校对处理过程。\n",
    "\n",
    "`__init()__`中的几个重要的输入\n",
    "- `dataset`: pytorch已有的数据读取接口，或者自定义的数据接口的输入，该输出要么是torch.utils.data.Dataset类的对象，要么是自定义的类的对象\n",
    "- `batch_size`: 一个batch输出数量的多少。\n",
    "- `shuffle`: 是否随机打乱顺序，一般在训练数据中进行打乱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 构建dataload，batch_size设置成64，只打乱train 的数据\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset,batch_size=64,shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset,batch_size=64,shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归\n",
    "\n",
    "回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。\n",
    "\n",
    "在机器学习领域中的大多数任务通常都与预测（prediction）有关。 当我们想预测一个数值时，就会涉及到回归问题。 常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、 0-1变量预测（预测是否贷款）。 \n",
    "\n",
    "为了解释线性回归，我们举一个实际的例子：需要根据贷款申请人的数据信息预测其是否有违约的可能，以此判断是否通过此项贷款。为了开发一个能预测是否有违约的可能的模型，我们需要收集一个真实的数据集。这个数据集包括了贷款申请人的各种数据信息。在机器学习的术语中，该数据集称为训练数据集（training data set） 或训练集（training set）。 每行数据（比如一个用户相对应的数据）称为样本（sample）， 也可以称为数据点（data point）或数据样本（data instance）。 我们把试图预测的目标（比如是否违约）称为标签（label）或目标（target）。 预测所依据的自变量（面积和房龄）称为特征（feature）。\n",
    "\n",
    "通常，我们使用$n$来表示数据集中的样本数。 对索引为$i$的样本，其输出表示为$\\mathbf{x}^{(i)} = [x_1^{(i)},\\dots,x_n^{(i)}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线形模型\n",
    "线性假设是指目标可以表示为特征的加权和:\n",
    "$$ logit = \\mathbf{x}^{(i)} \\cdot \\mathbf{w} + b $$ \n",
    "其中$ \\mathbf{w} = [w_1^{(i)},\\dots,w_n^{(i)}] $ 称为权重，权重决定了每个特征对我们预测值的影响。 b称为偏置（bias）、偏移量（offset）或截距（intercept）。 偏置是指当所有特征都取值为0时，预测值应该为多少。给定一个数据集，我们的目标是寻找模型的权重$\\mathbf{w}$和偏置$b$， 使得根据模型做出的预测大体合理。\n",
    "\n",
    "### 逻辑函数(Logistic，也称为sigmoid,logit)\n",
    "可以发现，线性假设输出值无界，所以我们需要把线形输出值映射到输出概率$[0,1]$之间，参考[逻辑回归ppt](https://ustc-ai-sgy.github.io/slides/2.5%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%921.pdf)sigmoid函数部分\n",
    "\n",
    "### 损失函数\n",
    "\n",
    "在我们开始考虑如何用模型拟合（fit）数据之前，我们需要确定一个拟合程度的度量。 损失函数（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。\n",
    "\n",
    "在这个实验部分，我们选择最大对数最大似然估计，对于所有权重（参数组合），我们选择一组参数，使得预测似然性最大。\n",
    "\n",
    "#### 极大似然估计\n",
    "在统计学中，极大似然估计（Maximum Likelihood Estimation）是用来估计模型参数的一种方法，就是利用已知样本的结果信息，反推出最有可能导致这样结果的模型参数值。简而言之，最大似然估计旨在找到能使已知数据最“自然”、最“合理”的模型参数。\n",
    "\n",
    "拓展阅读：[花书5.5](https://www.deeplearningbook.org/contents/ml.html)\n",
    "\n",
    "#### 对数最大似然估计\n",
    "\n",
    "由于本次实验相当于二分类问题，模型预测一个数据点为正样本的概率为:\n",
    "$$\\hat{y}_i = \\hat{P}_{positive}(i) = \\frac{1}{1+e^{ -logit}} $$\n",
    "预测其为负样本的概率为:\n",
    "$$\\hat{P}_{negetive}(i) = \\frac{e^{-logit}}{1+e^{ -logit}} $$\n",
    "\n",
    "结合最大似然估计，我们希望样本的预测概率尽可能接近真实标签。对于二分类问题，交叉熵损失函数（Cross-Entropy Loss）通常用于衡量预测概率与真实标签之间的差异。交叉熵损失函数定义如下：\n",
    "\n",
    "$$L(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$$\n",
    "\n",
    "其中，$y_i$ 是第 $i$ 个样本的真实标签，$\\hat{y}_i$ 是第 $i$ 个样本预测为正样本的概率，$N$ 是样本总数。\n",
    "\n",
    "\n",
    "#### torch.nn.BCELoss\n",
    "\n",
    "torch.nn.BCELoss 是PyTorch中用于计算二分类交叉熵损失的类。\n",
    "它的数学公式为:\n",
    "$$BCE(x, y) = -[y * log(x) + (1 - y) * log(1 - x)]$$\n",
    "\n",
    "其中:\n",
    "- x: 模型预测的概率值,范围在[0,1]之间\n",
    "- y: 真实标签,值为0或1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, norm='l2'):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        assert norm in ['l1', 'l2'] \n",
    "        self.w = nn.Parameter(torch.randn(input_dim, 1))\n",
    "        self.b = nn.Parameter(torch.randn(1))\n",
    "        self.norm=norm\n",
    "\n",
    "\n",
    "    def l1_norm(self):\n",
    "        # return l1 norm as a loss\n",
    "        # TODO\n",
    "        return torch.sum(torch.abs(self.w))\n",
    "\n",
    "    def l2_norm(self):\n",
    "        # return l2 norm as a loss\n",
    "        # TODO\n",
    "        return torch.sum(self.w ** 2)\n",
    "\n",
    "    def normnize(self):\n",
    "        # return l1 or l2 norm as a loss\n",
    "        if self.norm == 'l1':\n",
    "            return self.l1_norm()\n",
    "        else:\n",
    "            return self.l2_norm()\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # return sigmoid of x\n",
    "        # TODO\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return logits and call sigmoid function.\n",
    "        # TODO\n",
    "        logits = torch.mm(x, self.w) + self.b\n",
    "        probabilities = self.sigmoid(logits)\n",
    "        return logits, probabilities\n",
    "\n",
    "def criterion(y_pred, y_true):\n",
    "    # 计算二元交叉熵损失\n",
    "    # y_pred: 模型预测的概率值,范围在[0,1]之间\n",
    "    # y_true: 真实标签,值为0或1\n",
    "    # 返回: 损失值\n",
    "    # compute loss \n",
    "    loss_fn = nn.BCELoss()\n",
    "    loss = loss_fn(y_pred, y_true)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:00<00:13,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.9906268372535706\n",
      "Epoch: 0, Val Accuracy: 0.485\n",
      "Epoch: 1, Train Loss: 0.8314351530075074\n",
      "Epoch: 1, Val Accuracy: 0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:00<00:13,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 0.7489128003120422\n",
      "Epoch: 2, Val Accuracy: 0.593\n",
      "Epoch: 3, Train Loss: 0.7054599652290344\n",
      "Epoch: 3, Val Accuracy: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:00<00:13,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 0.6818426914215088\n",
      "Epoch: 4, Val Accuracy: 0.643\n",
      "Epoch: 5, Train Loss: 0.6711151871681214\n",
      "Epoch: 5, Val Accuracy: 0.639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:01<00:12,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 0.6651879634857177\n",
      "Epoch: 6, Val Accuracy: 0.648\n",
      "Epoch: 7, Train Loss: 0.6614953083992005\n",
      "Epoch: 7, Val Accuracy: 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:01<00:12,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: 0.657858690738678\n",
      "Epoch: 8, Val Accuracy: 0.654\n",
      "Epoch: 9, Train Loss: 0.6570672841072083\n",
      "Epoch: 9, Val Accuracy: 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:01<00:11,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 0.6558146471977234\n",
      "Epoch: 10, Val Accuracy: 0.653\n",
      "Epoch: 11, Train Loss: 0.6548971781730651\n",
      "Epoch: 11, Val Accuracy: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:01<00:12,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train Loss: 0.6542377362251282\n",
      "Epoch: 12, Val Accuracy: 0.653\n",
      "Epoch: 13, Train Loss: 0.6543659234046936\n",
      "Epoch: 13, Val Accuracy: 0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:02<00:12,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train Loss: 0.6544358477592468\n",
      "Epoch: 14, Val Accuracy: 0.661\n",
      "Epoch: 15, Train Loss: 0.6541555480957031\n",
      "Epoch: 15, Val Accuracy: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [00:02<00:11,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train Loss: 0.6535508131980896\n",
      "Epoch: 16, Val Accuracy: 0.637\n",
      "Epoch: 17, Train Loss: 0.6532485632896423\n",
      "Epoch: 17, Val Accuracy: 0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:02<00:11,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train Loss: 0.6534453115463257\n",
      "Epoch: 18, Val Accuracy: 0.652\n",
      "Epoch: 19, Train Loss: 0.6532463097572326\n",
      "Epoch: 19, Val Accuracy: 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:03<00:11,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train Loss: 0.6530176105499268\n",
      "Epoch: 20, Val Accuracy: 0.649\n",
      "Epoch: 21, Train Loss: 0.652724313735962\n",
      "Epoch: 21, Val Accuracy: 0.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:03<00:10,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train Loss: 0.6525092425346375\n",
      "Epoch: 22, Val Accuracy: 0.649\n",
      "Epoch: 23, Train Loss: 0.652461733341217\n",
      "Epoch: 23, Val Accuracy: 0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [00:03<00:10,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train Loss: 0.6525444259643555\n",
      "Epoch: 24, Val Accuracy: 0.637\n",
      "Epoch: 25, Train Loss: 0.6525601992607116\n",
      "Epoch: 25, Val Accuracy: 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [00:03<00:09,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train Loss: 0.6527120585441589\n",
      "Epoch: 26, Val Accuracy: 0.642\n",
      "Epoch: 27, Train Loss: 0.6523633251190185\n",
      "Epoch: 27, Val Accuracy: 0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:04<00:09,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train Loss: 0.6519721446037292\n",
      "Epoch: 28, Val Accuracy: 0.642\n",
      "Epoch: 29, Train Loss: 0.6521027941703796\n",
      "Epoch: 29, Val Accuracy: 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:04<00:09,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train Loss: 0.6514816770553589\n",
      "Epoch: 30, Val Accuracy: 0.633\n",
      "Epoch: 31, Train Loss: 0.6526574420928956\n",
      "Epoch: 31, Val Accuracy: 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [00:04<00:08,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Train Loss: 0.6521933059692383\n",
      "Epoch: 32, Val Accuracy: 0.652\n",
      "Epoch: 33, Train Loss: 0.6518853311538696\n",
      "Epoch: 33, Val Accuracy: 0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:04<00:08,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Train Loss: 0.6526990733146667\n",
      "Epoch: 34, Val Accuracy: 0.647\n",
      "Epoch: 35, Train Loss: 0.6520243940353394\n",
      "Epoch: 35, Val Accuracy: 0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [00:05<00:08,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Train Loss: 0.6523570494651795\n",
      "Epoch: 36, Val Accuracy: 0.636\n",
      "Epoch: 37, Train Loss: 0.6520781455039978\n",
      "Epoch: 37, Val Accuracy: 0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [00:05<00:08,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Train Loss: 0.6522872076034546\n",
      "Epoch: 38, Val Accuracy: 0.646\n",
      "Epoch: 39, Train Loss: 0.6517814869880676\n",
      "Epoch: 39, Val Accuracy: 0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:05<00:08,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Train Loss: 0.6516059055328369\n",
      "Epoch: 40, Val Accuracy: 0.648\n",
      "Epoch: 41, Train Loss: 0.6518449320793152\n",
      "Epoch: 41, Val Accuracy: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:06<00:08,  6.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Train Loss: 0.6522254452705384\n",
      "Epoch: 42, Val Accuracy: 0.642\n",
      "Epoch: 43, Train Loss: 0.6522830948829651\n",
      "Epoch: 43, Val Accuracy: 0.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:06<00:07,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Train Loss: 0.6522349467277527\n",
      "Epoch: 44, Val Accuracy: 0.648\n",
      "Epoch: 45, Train Loss: 0.6516287422180176\n",
      "Epoch: 45, Val Accuracy: 0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [00:06<00:07,  6.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Train Loss: 0.6520928344726562\n",
      "Epoch: 46, Val Accuracy: 0.634\n",
      "Epoch: 47, Train Loss: 0.6534364929199219\n",
      "Epoch: 47, Val Accuracy: 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:07<00:07,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Train Loss: 0.6516201496124268\n",
      "Epoch: 48, Val Accuracy: 0.654\n",
      "Epoch: 49, Train Loss: 0.6518954954147339\n",
      "Epoch: 49, Val Accuracy: 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [00:07<00:06,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Train Loss: 0.6518964791297912\n",
      "Epoch: 50, Val Accuracy: 0.636\n",
      "Epoch: 51, Train Loss: 0.6519841365814208\n",
      "Epoch: 51, Val Accuracy: 0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [00:07<00:06,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52, Train Loss: 0.6518551859855651\n",
      "Epoch: 52, Val Accuracy: 0.652\n",
      "Epoch: 53, Train Loss: 0.6519294657707214\n",
      "Epoch: 53, Val Accuracy: 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [00:07<00:06,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54, Train Loss: 0.6523740429878235\n",
      "Epoch: 54, Val Accuracy: 0.653\n",
      "Epoch: 55, Train Loss: 0.6525487942695618\n",
      "Epoch: 55, Val Accuracy: 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [00:08<00:05,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56, Train Loss: 0.6520836830139161\n",
      "Epoch: 56, Val Accuracy: 0.653\n",
      "Epoch: 57, Train Loss: 0.652132839679718\n",
      "Epoch: 57, Val Accuracy: 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:08<00:05,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58, Train Loss: 0.6517450971603393\n",
      "Epoch: 58, Val Accuracy: 0.656\n",
      "Epoch: 59, Train Loss: 0.6516727919578552\n",
      "Epoch: 59, Val Accuracy: 0.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:08<00:05,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60, Train Loss: 0.6518154449462891\n",
      "Epoch: 60, Val Accuracy: 0.646\n",
      "Epoch: 61, Train Loss: 0.6520733199119568\n",
      "Epoch: 61, Val Accuracy: 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [00:08<00:04,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62, Train Loss: 0.6519412813186646\n",
      "Epoch: 62, Val Accuracy: 0.642\n",
      "Epoch: 63, Train Loss: 0.6517405920028686\n",
      "Epoch: 63, Val Accuracy: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [00:09<00:04,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64, Train Loss: 0.6520083174705505\n",
      "Epoch: 64, Val Accuracy: 0.655\n",
      "Epoch: 65, Train Loss: 0.6517800040245056\n",
      "Epoch: 65, Val Accuracy: 0.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [00:09<00:04,  6.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66, Train Loss: 0.6522694263458252\n",
      "Epoch: 66, Val Accuracy: 0.64\n",
      "Epoch: 67, Train Loss: 0.6517991495132446\n",
      "Epoch: 67, Val Accuracy: 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [00:09<00:04,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68, Train Loss: 0.6519002819061279\n",
      "Epoch: 68, Val Accuracy: 0.637\n",
      "Epoch: 69, Train Loss: 0.6524657220840454\n",
      "Epoch: 69, Val Accuracy: 0.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [00:10<00:04,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70, Train Loss: 0.6517683963775635\n",
      "Epoch: 70, Val Accuracy: 0.643\n",
      "Epoch: 71, Train Loss: 0.6516208910942077\n",
      "Epoch: 71, Val Accuracy: 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [00:10<00:03,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72, Train Loss: 0.6528309230804443\n",
      "Epoch: 72, Val Accuracy: 0.634\n",
      "Epoch: 73, Train Loss: 0.6524542579650879\n",
      "Epoch: 73, Val Accuracy: 0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [00:10<00:03,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74, Train Loss: 0.6524649324417114\n",
      "Epoch: 74, Val Accuracy: 0.657\n",
      "Epoch: 75, Train Loss: 0.6517333216667175\n",
      "Epoch: 75, Val Accuracy: 0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [00:11<00:03,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76, Train Loss: 0.6521437430381775\n",
      "Epoch: 76, Val Accuracy: 0.642\n",
      "Epoch: 77, Train Loss: 0.651931128501892\n",
      "Epoch: 77, Val Accuracy: 0.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [00:11<00:02,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78, Train Loss: 0.6520233659744262\n",
      "Epoch: 78, Val Accuracy: 0.652\n",
      "Epoch: 79, Train Loss: 0.6520752944946289\n",
      "Epoch: 79, Val Accuracy: 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [00:11<00:02,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80, Train Loss: 0.6517534942626954\n",
      "Epoch: 80, Val Accuracy: 0.653\n",
      "Epoch: 81, Train Loss: 0.6521086659431458\n",
      "Epoch: 81, Val Accuracy: 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [00:11<00:02,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82, Train Loss: 0.6520183835029602\n",
      "Epoch: 82, Val Accuracy: 0.636\n",
      "Epoch: 83, Train Loss: 0.652304796218872\n",
      "Epoch: 83, Val Accuracy: 0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [00:12<00:01,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84, Train Loss: 0.6522074322700501\n",
      "Epoch: 84, Val Accuracy: 0.639\n",
      "Epoch: 85, Train Loss: 0.6519768490791321\n",
      "Epoch: 85, Val Accuracy: 0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [00:12<00:01,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86, Train Loss: 0.6521163558959961\n",
      "Epoch: 86, Val Accuracy: 0.637\n",
      "Epoch: 87, Train Loss: 0.6519476828575135\n",
      "Epoch: 87, Val Accuracy: 0.642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [00:12<00:01,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88, Train Loss: 0.6517349219322205\n",
      "Epoch: 88, Val Accuracy: 0.652\n",
      "Epoch: 89, Train Loss: 0.6520692944526673\n",
      "Epoch: 89, Val Accuracy: 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [00:13<00:01,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90, Train Loss: 0.6520102772712707\n",
      "Epoch: 90, Val Accuracy: 0.635\n",
      "Epoch: 91, Train Loss: 0.6520088887214661\n",
      "Epoch: 91, Val Accuracy: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [00:13<00:00,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92, Train Loss: 0.6520501952171326\n",
      "Epoch: 92, Val Accuracy: 0.638\n",
      "Epoch: 93, Train Loss: 0.6524877381324768\n",
      "Epoch: 93, Val Accuracy: 0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [00:13<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94, Train Loss: 0.6525750098228454\n",
      "Epoch: 94, Val Accuracy: 0.642\n",
      "Epoch: 95, Train Loss: 0.6524588646888733\n",
      "Epoch: 95, Val Accuracy: 0.651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [00:13<00:00,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96, Train Loss: 0.6522991223335266\n",
      "Epoch: 96, Val Accuracy: 0.643\n",
      "Epoch: 97, Train Loss: 0.6519968647956849\n",
      "Epoch: 97, Val Accuracy: 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98, Train Loss: 0.6521954460144043\n",
      "Epoch: 98, Val Accuracy: 0.651\n",
      "Epoch: 99, Train Loss: 0.651782386302948\n",
      "Epoch: 99, Val Accuracy: 0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "input_dim = train_input.size(1)\n",
    "model = LogisticRegression(input_dim, norm='l1')\n",
    "lr = 0.005\n",
    "num_epochs = 100\n",
    "norm_weight = 0.005\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    \n",
    "    val_total = 0\n",
    "    val_correct = 0\n",
    "    for val_input, val_label in val_loader:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _,val_pred = model(val_input)\n",
    "            val_pred = val_pred.squeeze()\n",
    "            predicted = (val_pred >= 0.5).float()\n",
    "            val_total += val_label.size(0)\n",
    "            val_correct += (predicted.view(-1) == val_label).sum().item()\n",
    "    val_accuracy = val_correct / val_total\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for train_input, train_label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits,pred = model(train_input)\n",
    "        pred = pred.squeeze()\n",
    "        loss = criterion(pred, train_label)\n",
    "        norm_loss = model.normnize()\n",
    "        loss = loss + norm_loss*norm_weight\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_avg = train_loss / len(train_loader)\n",
    "    print(f\"Epoch: {epoch}, Train Loss: {train_avg}\")\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Val Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.623\n"
     ]
    }
   ],
   "source": [
    "test_total = 0\n",
    "test_correct = 0\n",
    "for test_input, test_label in test_loader:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, test_pred = model(test_input)\n",
    "        predicted = (test_pred >= 0.5).float()\n",
    "        test_total += test_label.size(0)\n",
    "        test_correct += (predicted.view(-1) == test_label).sum().item()\n",
    "test_accuracy = test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logitic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
